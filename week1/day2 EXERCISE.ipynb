{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"deepseek-r1:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to understand these concepts related to large language models (LLMs), specifically neural networks, attention mechanisms, and transformers. Let me break it down step by step because some parts are a bit fuzzy for me.\n",
      "\n",
      "Starting with the neural network itself. From what I remember, a neural network is like a model inspired by how the human brain works. It's composed of layers of neurons, right? These layers include an input layer where data comes in, hidden layers that process the information, and an output layer that provides the final result. Each neuron in a layer connects to others and has weights and biases. The weights determine the importance of each connection, sort of like how much influence each input has on the output. Then there's activation functions involved which basically decide whether a neuron should be activated or not based on its inputs.\n",
      "\n",
      "Wait, so training a neural network means using data to adjust these weights and biases so that the model can make accurate predictions. This process uses optimization algorithms like gradient descent, right? And loss functions measure how far off the model's predictions are from actual outcomes. So during training, the model loses some sort of \"error\" which it minimizes over time.\n",
      "\n",
      "Now, moving on to attention mechanisms. I've heard this term thrown around in the context of NLP and LLMs. It seems like the key innovation behind models like Transformer. But what exactly is attention? From the definition, it's a way for models to focus on different parts of the input sequence when processing each element. Instead of considering every word equally regardless of position, attention allows the model to weigh some words more important than others.\n",
      "\n",
      "For example, if I'm translating a sentence, paying attention to certain words might help in correctly translating another part. The question is, how does this mechanism actually work? It must involve comparing different parts of the input vector representations somehow. Maybe it's calculating dot products or similarities between different word embeddings?\n",
      "\n",
      "Then there's the transformer architecture itself. So from what I know, traditional neural networks use recurrence for processing sequences because they can handle variable-length inputs by stepping through each token one by one. Transformers, however, process these sequences in parallel using self-attention mechanisms and position encodings.\n",
      "\n",
      "Position encoding is something new to me. It must help the model understand the order of tokens since transformers don't have a concept like word positions captured inherently due to their parallel processing. So they add some information that indicates where each token is positioned relative to others. That makes sense because without this, longer sequences might lose context.\n",
      "\n",
      "Putting it all together, I think in a transformer, you start with an input sequence of tokens, each converted into an embedding vector. Then, position encodings are added to these embeddings. The self-attention mechanism computes how important each token is relative to the others by considering their embeddings. This produces an attention map or scores which indicate the importance between tokens.\n",
      "\n",
      "After getting these attention scores, a linear transformation happens where each output vector (representing the processed information) is a combination of input vectors weighted by how central they are in that context. Finally, this transformed representation goes through a feed-forward neural network for further processing and learning.\n",
      "\n",
      "So summarizing, transformers use self-attention to model long-range dependencies effectively with parallel processing capabilities, which traditional RNNs or CNNs can't do as efficiently. The attention mechanism allows each position in the sequence to attend or focus on various other positions by computing a set of attention scores based on position-encoded inputs. This helps capture contextual information more accurately.\n",
      "\n",
      "Wait, but how is the self-attention calculated? I think it involves three vectors: Queries (Q), Keys (K), and Values (V). Each token in the sequence generates these Q, K, V vectors which are then used to compute attention scores via scaled dot-product. The scores are then normalized using a softmax function to give weights to different tokens' importance for each position.\n",
      "\n",
      "Then each output is a combination of weighted values from the entire sequence based on these scores. This allows the model to focus on what's relevant when processing each token, hence capturing longer-range dependencies as intended by the paper introducing transformers.\n",
      "\n",
      "Okay, so I think I'm getting closer to understanding how these components fit together in LLMs. The neural network is the overall framework, attention mechanics are a specific type of layer within that, and transformers use this mechanism along with self-attention and position encoding to process data more efficiently. This makes it possible to handle longer sequences and model interactions at various granularity levels better than before.\n",
      "\n",
      "But I still have some gaps in understanding, like how exactly the parameters or layers are structured during training or how the computational efficiency improves. Maybe that's something for another time when diving deeper into implementation details.\n",
      "</think>\n",
      "\n",
      "**Understanding Large Language Models: An Overview of Neural Networks, Attention Mechanisms, and Transformers**\n",
      "\n",
      "**1. Neural Networks:**\n",
      "A neural network is an artificial learning system inspired by the human brain. It consists of interconnected layers of nodes (neurons) that process information. The structure includes:\n",
      "\n",
      "- **Input Layer:** Receives raw data.\n",
      "- **Hidden Layers:** Process data through various computations using weights and biases.\n",
      "- **Output Layer:** Produces predictions or decisions.\n",
      "\n",
      "During training, the network adjusts its weights and biases to minimize prediction errors using optimization algorithms like gradient descent. Loss functions measure these errors, guiding the model towards better performance.\n",
      "\n",
      "**2. Attention Mechanisms:**\n",
      "Attention allows models to focus on relevant parts of input data when processing each element. Unlike traditional methods that consider all inputs equally regardless of position, attention mechanisms dynamically weight information based on its relevance.\n",
      "\n",
      "The mechanism typically involves:\n",
      "\n",
      "- **Queries (Q), Keys (K), and Values (V):** Vectors generated for each token.\n",
      "- **Scaled Dot-Product Attention:** Scores are computed via dot products between Q and K, then normalized with softmax to determine importance.\n",
      "\n",
      "This enables models to capture contextual dependencies and focus on significant information when processing sequences.\n",
      "\n",
      "**3. Transformer Architecture:**\n",
      "Transformers differ from RNNs by processing sequences in parallel using self-attention instead of recurrence.\n",
      "\n",
      "- **Self-Attention Mechanism:** Allows any position to attend to other positions, capturing dependencies within the input sequence.\n",
      "- **Position Encoding:** Added to embeddings to preserve positional information for tasks where token order matters but no inherent sequential understanding is present.\n",
      "  \n",
      "The transformer processes data with:\n",
      "\n",
      "1. **Token Embeddings:** Convert input tokens into dense vectors.\n",
      "2. **Position Encoding:** Added to embeddings to encode positional info.\n",
      "3. **Self-Attention:** Computes attention weights across the entire sequence, allowing focused processing.\n",
      "4. **Feed-Forward Neural Network:** Processes transformed representations through fully connected layers for final computation.\n",
      "\n",
      "This design enables efficient handling of variable-length inputs and modeling of long-range dependencies beyond RNN capabilities, as demonstrated in models like the Transformer architecture introduced by Vaswani et al.\n",
      "\n",
      "In summary, transformers effectively model context and dependencies using attention mechanisms within a neural network framework, enabling LLMs to process and generate text with high accuracy and efficiency.\n"
     ]
    }
   ],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:7b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
